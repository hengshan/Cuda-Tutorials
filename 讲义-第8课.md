# 第8课讲义：Cooperative Groups 基础

## 课前回顾 (1分钟)

前面的课程中，我们一直使用：
- `__syncthreads()` - 同步整个block的线程
- `__shfl_down_sync()` - warp内的通信

**问题**：
- `__syncthreads()` 太粗粒度（必须是整个block）
- 直接用warp函数太底层（硬编码32）
- 能否有更灵活的线程组织方式？

## 重要区别
- 数据tiling：优化内存访问，减少全局内存带宽
- 线程tiling：优化线程协作，提供灵活的线程组织

数据tiling + 线程tiling = 最佳性能

---

**Cooperative Groups（协作组）**：CUDA 9.0引入的现代线程协作API

---

## 核心概念速讲 (3分钟)

### 1. Cooperative Groups 是什么？

**传统方式的局限**：
```cuda
// 只能同步整个block
__syncthreads();

// warp大小硬编码
#define WARP_SIZE 32
val = __shfl_down_sync(0xffffffff, val, offset);
```

**Cooperative Groups方式**：
```cuda
#include <cooperative_groups.h>
namespace cg = cooperative_groups;

// 获取当前线程所在的block
cg::thread_block block = cg::this_thread_block();
block.sync();  // 等价于__syncthreads()

// 创建灵活的tile
auto tile = cg::tiled_partition<16>(block);  // 16个线程一组
tile.sync();  // 只同步这16个线程
```

**优势**：
- 显式表达意图（代码更清晰）
- 灵活的分组大小
- 类型安全
- 为未来硬件特性准备

---

### 2. 核心对象

#### thread_block - 整个Block
```cuda
cg::thread_block block = cg::this_thread_block();

// 常用方法
block.sync();              // 同步
block.thread_rank();       // 线程在block中的索引
block.size();              // block的总线程数
```

#### tiled_partition - 固定大小的Tile
```cuda
// 将block分成多个tile，每个SIZE个线程
auto tile = cg::tiled_partition<SIZE>(block);

// SIZE必须是2的幂：1, 2, 4, 8, 16, 32
tile.sync();               // 同步tile内的线程
tile.shfl_down(val, offset); // tile内shuffle
tile.thread_rank();        // 线程在tile中的索引
tile.size();               // tile大小
```

---

### 3. 用Cooperative Groups重写归约

**传统warp归约**（第5课）：
```cuda
__device__ int warpReduceMax(int val) {
    for (int offset = 16; offset > 0; offset >>= 1) {
        int neighbor = __shfl_down_sync(0xffffffff, val, offset);
        val = max(val, neighbor);
    }
    return val;
}
```

**Cooperative Groups版本**：
```cuda
template<int TILE_SIZE>
__device__ int tileReduceMax(cg::thread_block_tile<TILE_SIZE> tile, int val) {
    for (int offset = TILE_SIZE / 2; offset > 0; offset >>= 1) {
        int neighbor = tile.shfl_down(val, offset);
        val = max(val, neighbor);
    }
    return val;
}

// 使用
auto tile32 = cg::tiled_partition<32>(block);
int result = tileReduceMax(tile32, local_max);
```

**好处**：
- 可以改成tile16, tile8等，无需重写函数
- 不需要硬编码mask (0xffffffff)
- 编译器可以做更好的优化

---

### 4. 多级归约策略

**Cooperative Groups的威力**：
```cuda
__global__ void findMaxCG(int *data, int n, int *result) {
    cg::thread_block block = cg::this_thread_block();

    // 第1级：每个线程处理多个元素
    int local_max = INT_MIN;
    for (int i = blockIdx.x * blockDim.x + threadIdx.x;
         i < n;
         i += blockDim.x * gridDim.x) {
        local_max = max(local_max, data[i]);
    }

    // 第2级：tile内归约（warp级别）
    auto tile32 = cg::tiled_partition<32>(block);
    local_max = tileReduceMax(tile32, local_max);

    // 第3级：每个warp的第一个线程写shared memory
    __shared__ int shared[32];  // 最多32个warps
    int warp_id = threadIdx.x / 32;
    int lane_id = threadIdx.x % 32;

    if (lane_id == 0) {
        shared[warp_id] = local_max;
    }
    block.sync();

    // 第4级：第一个warp做最终归约
    if (threadIdx.x < 32) {
        int val = (threadIdx.x < (blockDim.x / 32)) ? shared[threadIdx.x] : INT_MIN;
        val = tileReduceMax(tile32, val);

        if (threadIdx.x == 0) {
            atomicMax(result, val);
        }
    }
}
```

---

### 5. Cooperative Groups的层次

```
cg::grid_group               // 整个Grid（需要特殊启动）
    ↓
cg::thread_block             // 一个Block的所有线程
    ↓
cg::tiled_partition<32>      // 32个线程（warp）
    ↓
cg::tiled_partition<16>      // 16个线程
    ↓
cg::tiled_partition<8>       // 8个线程
    ↓
cg::tiled_partition<4>       // 4个线程
```

**每一层都有**：
- `.sync()` - 同步
- `.thread_rank()` - 线程索引
- `.size()` - 组大小
- `.shfl_*()` - shuffle通信

---

## CG 函数

```cpp
// CG 版本
cg::thread_block_tile<16> half_warp = cg::tiled_partition<16>(block);
cg::reduce(half_warp, val, cg::plus<float>());  // 16 线程归约

// 等价于手写版本（width=16）
for (int offset = 8; offset > 0; offset >>= 1) {  // 从 8 开始！
    int neighbor = __shfl_down_sync(0xffff, val, offset, 16);  // width=16
    val = val + neighbor;
}

// ========== CG 版本 ==========
cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);
int warp_max = cg::reduce(warp, local_max, cg::greater<int>());

// ========== 编译器展开后 ≈ ==========
for (int offset = 16; offset > 0; offset >>= 1) {
    int neighbor = __shfl_down_sync(0xffffffff, local_max, offset);
    local_max = max(local_max, neighbor);
}

// 不同的归约操作
cg::reduce(warp, val, cg::plus<int>());     // 求和 (使用 +)
cg::reduce(warp, val, cg::greater<int>());  // 最大值 (使用 max)
cg::reduce(warp, val, cg::less<int>());     // 最小值 (使用 min)
cg::reduce(warp, val, cg::bit_and<int>());  // 按位与
cg::reduce(warp, val, cg::bit_or<int>());   // 按位或
cg::reduce(warp, val, cg::bit_xor<int>());  // 按位异或

```

---

## 性能预期

| 实现 | 代码复杂度 | 性能 | 灵活性 |
|------|-----------|------|--------|
| 第3课 atomicMax | ⭐ | 慢 | 低 |
| 第4课 shared memory | ⭐⭐⭐ | 中 | 中 |
| 第5课 warp shuffle | ⭐⭐⭐⭐ | 快 | 低 |
| **第8课 Cooperative Groups** | ⭐⭐⭐ | **快** | **高** |

**CG的优势**：
- 性能与第5课相当
- 代码更清晰易懂
- 灵活性更高（可以轻松改tile大小）

---

## 编译注意事项

```bash
# 需要指定C++11或更高
nvcc -std=c++11 -arch=sm_120 -o lesson08 0801-cooperative-groups-intro.cu

# 或者
nvcc -std=c++14 -arch=sm_120 -o lesson08 0801-cooperative-groups-intro.cu
```

**要求**：
- CUDA 9.0+
- C++11+
- Compute Capability 3.0+（RTX 5090完全支持）

---

## 本节课目标

学完后你应该能：
- ✅ 理解Cooperative Groups的设计理念
- ✅ 使用`thread_block`替代`__syncthreads()`
- ✅ 使用`tiled_partition`创建灵活的tile
- ✅ 用CG重写归约算法
- ✅ 理解CG与传统API的对比

---


**下一课预告**：第9课将学习Thread Block Clusters（跨block协作），这是Cooperative Groups的终极形态！

