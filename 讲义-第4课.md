# 第4课讲义：共享内存 + 归约优化

## 课前回顾 (1分钟)

上节课（第3课）我们学了：
- 统一内存：`cudaMallocManaged()` 简化内存管理
- 归约算法：用 `atomicMax()` 合并所有线程的结果

**问题**：atomicMax有性能瓶颈！所有线程都竞争一个变量。

---

## 今天要解决的问题 (1分钟)

**目标**：把上节课的代码加速 3-5倍！

**核心思路**：
1. 使用共享内存（比全局内存快20倍+）
2. Block内先归约，减少原子操作次数
3. 从 26万次原子操作 → 1024次原子操作

---

## 核心概念速讲 (3分钟)

### 1. GPU内存层次 (Memory Hierarchy)

```
速度从快到慢：
寄存器 (Registers)
    ↓ 最快，但容量极小，每线程私有
共享内存 (Shared Memory) ← 今天的主角！
    ↓ 很快(数个周期，在无冲突时为 1-2周期)，48KB，block内共享
L1/L2 缓存
    ↓ 自动管理
全局内存 (Global Memory)
    ↓ 慢(400-800周期)，但容量大(32GB)
```

**共享内存特点**：
- 速度：比全局内存快 20-30倍
- 容量：每个SM 48-164KB（根据GPU型号）
- 范围：同一个block内的线程可以共享
- 生命周期：block执行期间有效

**声明方式**：
```cuda
__shared__ int shared_data[256];  // 静态大小
extern __shared__ int shared_data[];  // 动态大小（推荐）
```

---

### 2. __syncthreads() 同步

**为什么需要同步？**

**错误示例**（没有同步）：
```cuda
shared_data[tid] = local_max;  // 线程0写完
// 线程1可能还没写完，线程0就开始读shared_data[tid+stride]！
if (shared_data[tid] < shared_data[tid+stride]) { ... }
```

**正确做法**：
```cuda
shared_data[tid] = local_max;
__syncthreads();  // 屏障：等所有线程都写完
// 现在安全了，可以读了
if (shared_data[tid] < shared_data[tid+stride]) { ... }
```

**关键规则**：
- 必须在条件语句外（所有线程都要执行到）
- 只能同步block内的线程（不能跨block）
- 有性能开销，不要滥用

---

### 3. 归约树 (Reduction Tree)

**串行归约** (CPU做法)：
```
[3, 7, 2, 9, 1, 5, 4, 8]
max = 3
max = max(max, 7) = 7
max = max(max, 2) = 7
... 需要7次比较
```

**并行归约树**（GPU做法）：
```
第0步: [3, 7, 2, 9, 1, 5, 4, 8]  (8个数)
       线程: 0  1  2  3  4  5  6  7

第1步: stride=4, 线程0-3活跃
       线程0: max(3,1)=3
       线程1: max(7,5)=7
       线程2: max(2,4)=4
       线程3: max(9,8)=9
       结果: [3, 7, 4, 9, -, -, -, -]

第2步: stride=2, 线程0-1活跃
       线程0: max(3,4)=4
       线程1: max(7,9)=9
       结果: [4, 9, -, -, -, -, -, -]

第3步: stride=1, 线程0活跃
       线程0: max(4,9)=9
       结果: [9, -, -, -, -, -, -, -]

答案在 shared_data[0] = 9
```

**复杂度**：
- 步数：log₂(N)，例如256个线程只需8步
- 对比：串行需要255步
- 并行效率：极高！

---

## 代码结构 (等会live coding的框架)

```cuda
__global__ void findMaxGPU_shared(int *data, int n, int *result) {
    // 1. 分配共享内存
    extern __shared__ int shared_data[];
    int tid = threadIdx.x;

    // 2. 每个线程加载数据到共享内存
    int local_max = INT_MIN;
    for (int i = gid; i < n; i += stride) {
        local_max = max(local_max, data[i]);
    }
    shared_data[tid] = local_max;
    __syncthreads();  // 同步1

    // 3. 归约树
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_data[tid] = max(shared_data[tid],
                                   shared_data[tid + s]);
        }
        __syncthreads();  // 同步2
    }

    // 4. 线程0更新全局结果
    if (tid == 0) {
        atomicMax(result, shared_data[0]);
    }
}

// 启动kernel时需要指定共享内存大小：
kernel<<<blocks, threads, threads * sizeof(int)>>>(...)
//                        ^^^^^^^^^^^^^^^^^^^^^^
//                        第3个参数：动态共享内存大小
```

---

## 性能对比预期

| 实现方式 | 原子操作次数 | 预期耗时 | 加速比 |
|---------|------------|---------|--------|
| 第3课朴素版 | 262,144次 | ~5ms | 10x |
| 第4课优化版 | 1,024次 | ~1.5ms | 33x |

**关键改进**：
- 原子操作减少 256倍（256 = threadsPerBlock）
- 使用共享内存加速block内通信

---

## 本节课的学习目标

学完这节课，你应该能：
- ✅ 理解GPU内存层次结构
- ✅ 会用 `__shared__` 声明共享内存
- ✅ 会用 `__syncthreads()` 同步线程
- ✅ 理解并实现归约树算法
- ✅ 知道如何指定动态共享内存大小

---

## 接下来：20分钟 Live Coding

我会从头写完整代码，重点讲解：
1. 共享内存声明和分配
2. 归约树的每一步
3. 同步点的位置和原因
4. 性能对比

准备好了吗？让我们开始！🚀
