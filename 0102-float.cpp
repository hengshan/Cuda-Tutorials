#include <iostream>
#include <bitset>
#include <cstdint>
#include <cstring> // memcpy
#include <cmath>
#include <iomanip> // std::setprecision

// 打印 float 的 IEEE-754 结构
void printFloatIEEE754(float f) {
    uint32_t bits;
    std::memcpy(&bits, &f, sizeof(float)); // float → 原始 32 位二进制

    // 拆分三部分
    uint32_t sign     = (bits >> 31) & 0x1;
    uint32_t exponent = (bits >> 23) & 0xFF;
    uint32_t mantissa = bits & 0x7FFFFF;

    std::cout << "原始数值: " << f << "\n";
    std::cout << "二进制:   " << std::bitset<32>(bits) << "\n";
    std::cout << "符号位:   " << sign << "\n";
    std::cout << "指数:     " << std::bitset<8>(exponent)
              << " (十进制: " << exponent << ")\n";
    std::cout << "尾数:     " << std::bitset<23>(mantissa) << "\n";

    // 还原实际指数（减去偏移 127）
    int realExp = (int)exponent - 127;
    std::cout << "实际指数: " << realExp << "\n";

    // 还原近似值
    double m = 1.0;
    for (int i = 22; i >= 0; --i) {
        if (mantissa & (1u << i)) {
            m += std::pow(2, i - 23);
        }
    }
    double value = (sign ? -1 : 1) * m * std::pow(2, realExp);
    std::cout << std::fixed << std::setprecision(10);
    std::cout << "还原近似值: " << value << "\n";
}

int main() {
    float f = 3.14f;
    printFloatIEEE754(f);
    return 0;
}

// double (64-bit, 双精度)
// 总位数: 64
// 编码布局: 1位符号位 (S) | 11位指数位 (E) | 52位尾数位 (M)
// 指数偏移 (Bias): 1023
// 实际值计算公式: (-1)^S * (1 + M) * 2^(E - 1023)

// FP32 (Full Precision)
// 完全等同于C++的 float。
// 总位数: 32
// 布局: S1 | E8 | M23
// 用途: 通用计算，需要高精度的科学计算，神经网络训练的一部分（如梯度更新）。

// FP16 (Half Precision)
// 总位数: 16
// 编码布局: 1位符号位 (S) | 5位指数位 (E) | 10位尾数位 (M)
// 指数偏移 (Bias): 15
// 实际值计算公式: (-1)^S * (1 + M) * 2^(E - 15)
// 动态范围急剧缩小: 指数位只有5位，能表示的最大指数是 30 - 15 = 15，最小是 1 - 15 = -14。很容易出现上溢(overflow)和下溢(underflow)。
// 精度显著降低: 尾数只有10位，精度约为小数点后3-4位十进制数。
// 用途: 深度神经网络训练和推理的主力。在大部分计算中精度足够，且速度快、功耗低、占用显存和带宽减半。

// TF32 (TensorFloat-32)
// 这是NVIDIA为Ampere架构引入的“混合”格式，专为矩阵乘累加（MMA）操作设计。
// 总位数: 19位 (但存储在32位的容器中)
// 编码布局: 1位符号位 (S) | 8位指数位 (E) | 10位尾数位 (M)
// 指数偏移 (Bias): 127 (与FP32相同)
// 与FP32/FP16对比:
// 拥有和FP32一样的指数范围 (8位，偏移127)。这解决了FP16容易溢出的问题。
// 拥有和FP16一样的尾数精度 (10位)。精度略低于FP32，但远高于FP16。
// 工作原理: 在A100 GPU的Tensor Core上进行矩阵乘法时，输入数据可以是FP16、BF16或FP32，但会自动转换为TF32格式进行核心计算，最终输出可以是FP32或其他格式。对开发者基本透明。
// 用途: AI训练的默认精度。在不修改代码或超参数的情况下，能自动获得相比FP32更快的速度，且避免了FP16的溢出问题。

// FP8 (8-bit Floating Point)
// 这是NVIDIA为Hopper架构引入的最新格式，进一步优化AI性能。
// 总位数: 8
// 有两种子类型 (由用户选择或框架自动选择):
// E5M2： 1位符号位 | 5位指数位 | 2位尾数位
// 动态范围大，适合表示梯度等可能值范围很大的数据。
// E4M3： 1位符号位 | 4位指数位 | 3位尾数位
// 精度稍高，但范围较小，适合表示权重等数据。
// 用途: 大规模AI训练和推理。相比FP16，内存占用和带宽需求再减半，是训练万亿参数大模型的关键技术之一。

// BF16 (Brain Float 16)
// 设计哲学：“用FP32的范围，FP16的占用”。它牺牲了一些精度来换取极大的数值表示范围，从而避免在训练深度网络时出现梯度下溢/溢出问题。
// 编码方式 (总16位)：
// 1位符号位 (S)： 与FP32相同。
// 8位指数位 (E)： 与FP32的指数位宽完全相同。这是其核心特点。
// 7位尾数位 (M)： 比FP16（10位）还少3位，意味着精度更低。
// 指数偏移 (Bias)： 127 (与FP32相同)。

// 特性	    BF16 (bfloat16)	            FP16 (half)	        FP32 (float)
// 动态范围	非常宽 (同FP32)	            窄	                宽
// 精度	    低 (≈2-3位十进制)	        中 (≈3-4位十进制)	高 (≈7-8位十进制)
// 内存占用	2字节	                    2字节	            4字节
// 设计目的	保持范围，牺牲精度	        平衡范围和精度	    高精度通用计算

// INT8 (8-bit Integer)
// INT8使用8位二进制数来表示一个整数，通常采用二的补码形式。
// 设计哲学：极致压缩，专为推理加速。将模型权重和激活值从浮点（FP32/FP16）压缩到8位整数，以换取极致的速度和能效提升。
//
// 编码方式 (总8位)：
// 表示一个整数，范围通常在 -128 到 127 之间（有符号）或 0 到 255 之间（无符号）。
// 没有指数和尾数的概念，只有离散的整数值。
//
// 核心挑战：量化
// 直接将FP32模型转换为INT8会导致巨大精度损失，模型几乎无法工作。因此，INT8的使用完全依赖于一项称为量化的技术。
// 量化的本质是找到一个映射函数，将FP32数值范围的数值线性或非线性地映射到INT8的有限离散值上。
//
// 公式： Q = round(R / S) + Z
// R：真实的FP32值。
// Q：量化后的INT8值。
// S：缩放因子（scale），一个FP32数值，决定映射的“步长”。
// Z：零点（zero point），一个INT8值，通常对应真实值中的0。
//
// 为什么INT8对AI推理如此重要？
// 巨大的性能提升：
// 内存和带宽减少：模型大小减少为FP32的 1/4。从内存中加载权重和数据的带宽需求也降为1/4。
// 计算速度加快：GPU的Tensor Core对INT8有专门优化，其INT8的运算吞吐量（TOPS）通常是FP16的2到4倍。
// 功耗降低：移动端和嵌入式设备（如手机、自动驾驶芯片）的算力和电量有限，INT8模型是其上部署AI的唯一可行方案。
//
// 主要挑战：
// 精度损失：量化过程必然引入误差。对于敏感的模型，需要复杂的量化感知训练来弥补精度损失，而不是简单地在训练后转换。
// 调试困难：模型在INT8下的行为与FP32不同，出现问题时调试更复杂。
// 主要用途：深度学习推理。几乎所有的云端和边缘AI推理服务（如人脸识别、语音助手、图像分类）都大量使用INT8来加速并降低成本。它一般不用于训练。
