# 第7课讲义：矩阵乘法 - Tiling优化

## 课前回顾 (1分钟)

上节课（第6课）我们：
- 实现了朴素版矩阵乘法
- 发现问题：数据被重复读取，浪费带宽

**今天目标**：？倍加速！

---

## 今天要解决的问题 (1分钟)

**上节课的问题**：
```
A[row][k] 被同一行的线程重复读了N次
B[k][col] 被同一列的线程重复读了N次
```

**解决思路**：
- 把重复用的数据缓存到共享内存
- 一次读取，多次使用
- 减少全局内存访问16倍！

---

## 核心概念速讲 (3分钟)

### 1. Tiling（分块）思想

**把大矩阵分成小块（tiles）处理**：

```
原始矩阵（1024×1024）太大，分成16×16的小块：

A矩阵:          B矩阵:          C矩阵:
┌─┬─┬─┬─┐      ┌─┬─┬─┬─┐      ┌─┬─┬─┬─┐
├─┼─┼─┼─┤      ├─┼─┼─┼─┤      ├─┼─┼─┼─┤
├─┼─┼─┼─┤  ×   ├─┼─┼─┼─┤  =   ├─┼─┼─┼─┤
└─┴─┴─┴─┘      └─┴─┴─┴─┘      └─┴─┴─┴─┘
  64×64块        64×64块        64×64块
```

**每个tile = 16×16 = 256个元素 = 刚好一个block的线程数！**

---

### 2. Tile加载策略

**计算C的一个tile需要：**
```
C_tile[i][j] = Σ A_tile[i][t] × B_tile[t][j]
               t
```

**逐步加载tiles**（假设K有4个tiles）：
```
步骤1: 加载 A_tile0 和 B_tile0 → 计算部分和1
步骤2: 加载 A_tile1 和 B_tile1 → 计算部分和2
步骤3: 加载 A_tile2 和 B_tile2 → 计算部分和3
步骤4: 加载 A_tile3 和 B_tile3 → 计算部分和4
最后:  sum = 部分和1 + 部分和2 + 部分和3 + 部分和4
```

---

### 3. 共享内存缓存

**为每个block分配共享内存**：
```cuda
__shared__ float As[16][16];  // 缓存A的tile
__shared__ float Bs[16][16];  // 缓存B的tile
```

**数据流**：
```
全局内存(慢) → 共享内存(快) → 计算
     ↓              ↓
 读1次          读16次（被重用！）
```

---

### 4. 数据重用分析

**朴素版**（第6课）：
- A[row][k]：从全局内存读1024次（K=1024）
- 总读取：每个线程 2K 次全局访问

**Tiling版**（今天）：
- A_tile加载1次到共享内存
- 从共享内存读16次
- 全局内存访问减少：1024 → 1024/16 = 64次
- **减少16倍！**

**为什么是16倍？**
- TILE_SIZE = 16
- 每个tile被重用TILE_SIZE次

---

## 今天的代码结构

```cuda
#define TILE_SIZE 16

__global__ void matmul_tiled(float *A, float *B, float *C,
                             int M, int N, int K) {
    // 线程和block索引
    int tx = threadIdx.x, ty = threadIdx.y;
    int row = blockIdx.y * TILE_SIZE + ty;
    int col = blockIdx.x * TILE_SIZE + tx;

    // 共享内存
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    float sum = 0.0f;

    // 遍历所有tiles
    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;
    for (int t = 0; t < numTiles; t++) {
        // 步骤1: 加载tile到共享内存
        As[ty][tx] = A[row * K + t*TILE_SIZE + tx];
        Bs[ty][tx] = B[(t*TILE_SIZE + ty) * N + col];
        __syncthreads();  // 等所有线程加载完

        // 步骤2: 使用共享内存计算
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[ty][k] * Bs[k][tx];
        }
        __syncthreads();  // 等所有线程用完再加载下一个tile
    }

    // 步骤3: 写回结果
    C[row * N + col] = sum;
}
```

---

## 关键点解析

### 为什么需要两次__syncthreads()？

**第一次同步**：
```cuda
As[ty][tx] = A[...];  // 所有线程加载数据
__syncthreads();      // 必须等大家都加载完！
for (int k = 0; k < TILE_SIZE; k++) {
    sum += As[ty][k] * Bs[k][tx];  // 才能安全读取
}
```
如果不同步，有的线程可能读到未初始化的数据！

**第二次同步**：
```cuda
for (int k = 0; k < TILE_SIZE; k++) {
    sum += As[ty][k] * Bs[k][tx];  // 所有线程用完数据
}
__syncthreads();      // 必须等大家都用完！
// 下一次循环会覆盖As和Bs
```
如果不同步，有的线程可能还没用完，As就被覆盖了！

---

## 与硬件峰值对比
**为什么还是这么低？**
1. 通用矩阵乘法很难优化
2. cuBLAS用了Tensor Cores（FP16/INT8）能到60-70%
3. 我们的实现已经很不错了！（学习目的）

**进一步优化方向**：
- 更大的tile size（32×32）
- 向量化访问（float4）
- Tensor Cores（需要FP16）
- 双缓冲（重叠计算与访存）

---

## 本节课目标

学完后你应该能：
- ✅ 理解Tiling算法思想
- ✅ 使用共享内存做缓存
- ✅ 知道两次同步的必要性
- ✅ 计算数据重用率
- ✅ 达到实用级别的性能

---

## 接下来：20分钟 Live Coding

重点：
1. Tile加载逻辑
2. 两次__syncthreads()位置
3. 边界处理（K不是TILE_SIZE倍数）


