# 第9课讲义：Thread Block Clusters（线程块簇）

## 课前回顾 (1分钟)

第8课学了Cooperative Groups：
- `thread_block` - block内协作
- `tiled_partition` - warp内协作

**问题**：能否让**不同block之间协作**？

**传统限制**：
- 不同block无法同步
- 不同block的shared memory互不可见
- 只能通过global memory通信（慢）

---

## 核心概念速讲 (3分钟)

### 1. 什么是Thread Block Cluster？

**层次结构**：
```
Grid（整个kernel）
  └─ Cluster（多个block的组）
      └─ Block（多个线程）
          └─ Warp（32个线程）
              └─ Thread（单个线程）
```

**示例**：
```cuda
// 传统：Grid = 很多个独立的block
kernel<<<100, 256>>>();  // 100个block，互不协作

// Clusters：Grid = 多个cluster，每个cluster包含多个block
dim3 grid(100, 1, 1);
dim3 block(256, 1, 1);
dim3 cluster(4, 1, 1);  // 每个cluster有4个block

kernel<<<grid, block, 0, 0, cluster>>>();
// 100个block分成25个cluster，每个cluster 4个block
```

### 2. 为什么需要Clusters？

**传统方法：通过Global Memory通信**
```cuda
// Block 0写
global_buffer[idx] = data;
__threadfence();  // 确保写入可见
atomicAdd(&flag, 1);  // 通知其他block

// Block 1读（需要自旋等待）
while (atomicAdd(&flag, 0) < expected) { /* 等待 */ }
int value = global_buffer[idx];  // 读取

// 问题：
// - 需要global memory（慢，~400-800 cycle延迟）
// - 需要atomic（慢）
// - 需要自旋等待（浪费）
```
---

### 3. Cluster的关键特性

#### 特性1：Cluster内同步
```cuda
#include <cuda/cluster>

__global__ void __cluster_dims__(4, 1, 1)  // 声明cluster大小
clusterKernel(...) {
    namespace cg = cooperative_groups;

    // 获取cluster对象
    cg::cluster_group cluster = cg::this_cluster();

    // 同步整个cluster的所有block
    cluster.sync();

    // 现在所有block都到达了这个点
}
```

#### 特性2：分布式共享内存（Distributed Shared Memory）
```cuda
__global__ void __cluster_dims__(4, 1, 1)
clusterKernel(...) {
    __shared__ int local_data[256];  // 本block的shared memory

    // 访问cluster内其他block的shared memory
    int *remote_data = cluster.map_shared_rank(local_data, 1);
    // remote_data指向cluster中rank 1的block的shared memory

    // 读取其他block的数据！
    int value = remote_data[threadIdx.x];
}
```
---

### 4. Cluster的硬件支持

**限制**：
- Cluster大小：通常1-8个block
- 所有cluster必须大小相同
- 需要足够的SM资源（shared memory, registers）

---

### 5. Cluster编程模型

```cuda
__global__ void __cluster_dims__(4, 1, 1)  // cluster: 4×1×1
matmulCluster(float *A, float *B, float *C, int N) {
    cg::cluster_group cluster = cg::this_cluster();

    // 获取cluster内的位置
    int cluster_rank = cluster.block_rank();     // 我是cluster中的第几个block (0-3)
    int num_blocks = cluster.num_blocks();       // cluster总共有几个block (4)

    // 分布式shared memory
    __shared__ float tile[TILE_SIZE][TILE_SIZE];

    // 每个block加载不同的tile
    load_tile(tile, A, cluster_rank);

    cluster.sync();  // 确保所有block都加载完

    // 访问其他block的tile
    for (int i = 0; i < num_blocks; i++) {
        float *remote_tile = cluster.map_shared_rank(tile, i);
        // 使用remote_tile计算...
    }

    cluster.sync();  // 计算完成前同步
}
```



---

## 今天的代码结构

```cuda
#include <cooperative_groups.h>
#include <cuda/cluster>

// 声明cluster维度
__global__ void __cluster_dims__(4, 1, 1)
clusterReduceSum(int *data, int *output, int n) {
    namespace cg = cooperative_groups;

    // 1. 获取cluster对象
    cg::cluster_group cluster = cg::this_cluster();
    int cluster_rank = cluster.block_rank();

    // 2. 每个block计算部分和
    __shared__ int partial_sum;
    // ... 计算 ...

    cluster.sync();  // 同步

    // 3. Block 0收集其他block的结果
    if (cluster_rank == 0) {
        int total = partial_sum;  // 自己的

        // 读取其他block的shared memory
        for (int i = 1; i < cluster.num_blocks(); i++) {
            int *remote = cluster.map_shared_rank(&partial_sum, i);
            total += *remote;
        }

        // 写结果
        if (threadIdx.x == 0) {
            output[blockIdx.x / cluster.num_blocks()] = total;
        }
    }
}
```

---

## 常见陷阱

### 1. Cluster大小限制
```cuda
// ✗ 太大（硬件限制）
__global__ void __cluster_dims__(32, 1, 1) kernel() {}

// ✓ 合理范围：1-8
__global__ void __cluster_dims__(4, 1, 1) kernel() {}
```

### 2. 资源不足
```cuda
// 如果shared memory用太多，cluster可能无法调度
__shared__ float huge[10000];  // 可能导致cluster无法形成
```

### 3. 忘记同步
```cuda
data[i] = value;
// cluster.sync();  ← 缺少同步！
int *remote = cluster.map_shared_rank(data, 1);
// 可能读到未初始化的数据
```
# CUDA 同步与内存访问延迟对比表

> 时钟周期为近似值，实际延迟受负载、竞争等因素影响

## 一、内存访问延迟

| 内存类型 | 延迟（时钟周期） | 说明 |
|----------|------------------|------|
| Register | 1 | 寄存器访问，零开销 |
| Shared Memory | 20-30 | SM 内部，bank conflict 时更高 |
| DSMEM (Cluster 内跨 Block) | 30-50 | SM 互连，不经过 L2 |
| L1 Cache Hit | 30-40 | SM 本地 L1 |
| L2 Cache Hit | 150-200 | 片上共享 L2 |
| Global Memory (L2 Miss) | 400-800 | 访问 HBM/GDDR |
| Global Memory Atomic (L2 Hit) | 200-400 | 原子操作，L2 命中 |
| Global Memory Atomic (L2 Miss) | 400-800 | 原子操作，需访问 DRAM |

## 二、同步机制延迟

| 同步机制 | 延迟（时钟周期） | 范围 | 实现方式 |
|----------|------------------|------|----------|
| `__syncwarp()` | ~5 | Warp 内 32 线程 | 硬件指令 |
| `__syncthreads()` | 20-50 | Block 内所有线程 | 硬件 Barrier |
| `cluster.sync()` | 100-300 | Cluster 内所有 Block | 硬件 Barrier (Hopper+) |
| `grid.sync()` | 1000-5000+ | Grid 内所有 Block | 软件实现（全局内存 + Atomic） |
| Kernel Launch | 5-30 μs | N/A | CPU-GPU 交互 |
| cudaStreamSynchronize | 10-50 μs | Stream 级别 | CPU-GPU 交互 |
| cudaDeviceSynchronize | 10-100 μs | 设备级别 | CPU-GPU 交互 |

## 三、Cluster vs Grid 同步对比

| 特性 | Cluster Sync | Grid Sync |
|------|--------------|-----------|
| 延迟 | 100-300 周期 | 1000-5000+ 周期 |
| 范围 | 2-16 个 Block（同 GPC） | 所有 Block |
| 实现 | 硬件原生 Barrier | 软件（Atomic + Memory Fence） |
| 可扩展性 | 固定，受 Cluster 大小限制 | 任意 Block 数量 |
| 架构要求 | Hopper (SM90) 及以上 | Volta (SM70) 及以上 |

## 四、常见操作延迟速查

| 操作 | 延迟 | 备注 |
|------|------|------|
| 一次 DSMEM 读取 | ~30 周期 | Cluster 内跨 Block |
| 一次 Global Atomic Add | ~400 周期 | 竞争时更高 |
| Cluster Barrier | ~200 周期 | 等待最慢的 Block |
| Grid Barrier | ~2000 周期 | Block 数量多时更高 |
| 新 Kernel 启动 | ~10 μs | 约 15000-20000 周期 @1.5GHz |

## 五、优化建议

```
性能排序（从快到慢）：

Warp 内通信      ████                    (~5 cycles)
Block 内同步     ████████                (~30 cycles)
DSMEM 访问       ████████                (~30 cycles)
Cluster 同步     ████████████████        (~200 cycles)
L2 访问          ████████████████████    (~200 cycles)
Global Atomic    ████████████████████████████████    (~400 cycles)
Grid 同步        ████████████████████████████████████████████████  (~2000+ cycles)
Kernel 启动      ████████████████████████████████████████████████████████ (~15000+ cycles)
```

### 最佳实践

1. **批量操作后同步**：减少同步次数，摊销同步成本
2. **避免频繁 Grid Sync**：考虑拆分为多个 Kernel
3. **利用 DSMEM**：Cluster 内跨 Block 通信比全局内存快 10x+

---

## 本节课目标

学完后你应该能：
- ✅ 理解Thread Block Clusters的概念
- ✅ 使用`__cluster_dims__`声明cluster
- ✅ 使用`cluster_group`对象
- ✅ 实现分布式共享内存访问
- ✅ 掌握cluster内同步

---

## 接下来：20分钟 Live Coding

重点：
1. 声明cluster维度
2. 获取cluster对象
3. 使用分布式shared memory
4. 实现跨block归约
5. 性能对比
